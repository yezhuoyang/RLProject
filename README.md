# RLProject
This is the big project of my Reinforcement learning courses.



# Reference 


* Janner, Michael, et al. "When to trust your model: Model-based policy optimization." arXiv preprint arXiv:1906.08253 (2019).

* Chua, Kurtland, et al. "Deep reinforcement learning in a handful of trials using probabilistic dynamics models." arXiv preprint arXiv:1805.12114 (2018). 

* Buckman, Jacob, et al. "Sample-efficient reinforcement learning with stochastic ensemble value expansion." arXiv preprint arXiv:1807.01675 (2018).

* Rashid, Tabish, et al. "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning." arXiv preprint arXiv:1803.11485 (2018).

* Lowe, Ryan, et al. "Multi-agent actor-critic for mixed cooperative-competitive environments." arXiv preprint arXiv:1706.02275 (2017).

* Sukhbaatar, Sainbayar, Arthur Szlam, and Rob Fergus. "Learning multiagent communication with backpropagation." arXiv preprint arXiv:1605.07736 (2016).

* Kumar, Aviral, et al. "Conservative q-learning for offline reinforcement learning." arXiv preprint arXiv:2006.04779 (2020).

* Yu, Tianhe, et al. "Mopo: Model-based offline policy optimization." arXiv preprint arXiv:2005.13239 (2020).

* Kumar, Aviral, et al. "Stabilizing off-policy q-learning via bootstrapping error reduction." arXiv preprint arXiv:1906.00949 (2019).
